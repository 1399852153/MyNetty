~~# 从零开始实现简易版Netty(十) MyNetty 自定义编解码器解决TCP黏包/拆包问题
## 1. TCP黏包拆包问题介绍
在上一篇博客中，截止lab9版本MyNetty已经实现了包括池化内存容器在内的绝大多数功能。按照计划，lab10也是本系列博客的最后一个迭代中将实现通用的编解码处理器来解决接受数据时的tcp黏包/拆包问题。  
由于本文属于系列博客，读者需要对之前的博客内容有所了解才能更好地理解本文内容。
* lab1版本博客：[从零开始实现简易版Netty(一) MyNetty Reactor模式](https://www.cnblogs.com/xiaoxiongcanguan/p/18939320)
* lab2版本博客：[从零开始实现简易版Netty(二) MyNetty pipeline流水线](https://www.cnblogs.com/xiaoxiongcanguan/p/18964326)
* lab3版本博客：[从零开始实现简易版Netty(三) MyNetty 高效的数据读取实现](https://www.cnblogs.com/xiaoxiongcanguan/p/18979699)
* lab4版本博客：[从零开始实现简易版Netty(四) MyNetty 高效的数据写出实现](https://www.cnblogs.com/xiaoxiongcanguan/p/18992091)
* lab5版本博客：[从零开始实现简易版Netty(五) MyNetty FastThreadLocal实现](https://www.cnblogs.com/xiaoxiongcanguan/p/19005381)
* lab6版本博客：[从零开始实现简易版Netty(六) MyNetty ByteBuf实现](https://www.cnblogs.com/xiaoxiongcanguan/p/19029215)
* lab7版本博客：[从零开始实现简易版Netty(七) MyNetty 实现Normal规格的池化内存分配](https://www.cnblogs.com/xiaoxiongcanguan/p/19084677)
* lab8版本博客：[从零开始实现简易版Netty(八) MyNetty 实现Small规格的池化内存分配](https://www.cnblogs.com/xiaoxiongcanguan/p/19109991)
* lab9版本博客：[从零开始实现简易版Netty(九) MyNetty 实现池化内存的线程本地缓存](https://www.cnblogs.com/xiaoxiongcanguan/p/19148861)
#####
操作系统实现的传输层tcp协议中，向上层的应用保证尽最大可能的(best effort delivery)、可靠的传输字节流，但并不关心实际传输的数据包是否总是符合应用层的要求。   
应用层有时候会在短时间内向对端发送N个业务逻辑上独立的请求，而操作系统tcp层面出于效率的考虑并不会按照应用层的逻辑划分一个一个独立的进行消息的发送，而是会基于当前的网络负载尽可能的多的将消息数据批量发送。这使得我们在EventLoop事件循环中read读取到的数据并不总是完整的，符合应用层逻辑划分的消息数据。  
#####
* **黏包问题：** 假设应用层发送的一次请求数据量比较小(比如0.1kb)，tcp层可能不会在接到应用请求后立即进行传输，而是会稍微等待一小会。
  这样如果应用层在短时间内需要传输多次0.1kb的请求，就可以攒在一起批量传输，传输效率会高很多。
  但这带来的问题就是接收端一次接受到的数据包内应用程序逻辑上的多次请求**黏连**在了一起，需要通过一些方法来将其拆分还原为一个个独立的信息给应用层。
* **拆包问题：** 假设应用层发送的一次请求数据量比较大(比如100Mb)，而tcp层的数据包容量的最大值是有限的，所以应用层较大的一次请求数据会被**拆分**为多个包分开发送。
  这就导致接收端接受到的某个数据包其实并不是完整的应用层请求数据，没法直接交给应用程序去使用，
  而必须等待后续对应请求的所有数据包都接受完成后，才能组装成完整的请求对象再交给应用层处理。
#####
当然，导致黏包拆包的场景远不止上述的那么单一，整体的网络负载变化等都可能导致黏包/拆包的现象。    
**可以说，黏包/拆包问题并不能看做是tcp的问题，而是应用层最终需求与tcp传输层功能不匹配导致的问题。**   
tcp出于传输效率的考虑无法很好的解决这个问题，所以黏包拆包问题最终只能在更上面的应用层自己来处理。  
##### 黏包拆包示意图
接受到的一个数据包中可能同时存在黏包问题和拆包问题(如下图所示)
#####
![img.png](img.png)

##### 黏包/拆包问题解决方案
解决黏包/拆包问题最核心的思路是，如何知道一个应用层完整请求的边界。
对于黏包问题，基于边界可以独立的拆分出每一个请求；对于拆包问题，如果发现收到的数据包末尾没有边界，则继续等待新的数据包，逐渐累积直到发现边界后再一并上交给应用程序。
#####
主流的解决黏包拆包的应用层协议设计方案有三种：
#####
|                    | 介绍                                                  | 优点                           | 缺点                                       |
|--------------------|-----------------------------------------------------|------------------------------|------------------------------------------|
| 1.基于固定长度的协议        | 每个消息都是固定的大小，如果实际上小于固定值，则需要填充                        | 简单;易于实现                      | 固定值过大，填充会浪费大量传输带宽；固定值过小则限制了可用的消息体大小      |
| 2.基于特殊分隔符的协议       | 约定一个特殊的分隔符，以这个分割符为消息边界                              | 简单;且消息体长度是可变的，性能好            | 消息体的业务数据不允许包含这个特殊分隔符，否则会错误的拆分数据包。因此兼容性较差 |
| 3.基于业务数据长度编码的协议    | 设计一个固定大小的消息请求头(比如固定16字节、20字节大小)，在消息请求头中包含实际的业务消息体长度 | 消息体长度可变，性能好；对业务数据内容无限制，兼容性也好 | 实现起来稍显复杂                                 |

#####
上述这段关于黏包/拆包问题的内容基本copy自我2年前的关于手写简易rpc框架的博客：[自己动手实现rpc框架(一) 实现点对点的rpc通信](https://www.cnblogs.com/xiaoxiongcanguan/p/17506728.html)。  
只是在当时，我仅仅是一个对Netty不甚了解的使用者，简单的使用Netty来实现rpc框架中基本的网络通信功能，并通过MessageToByteEncoder/ByteToMessageDecoder设计通信协议来处理黏包拆包问题。   
而现在却尝试着参考Netty的源码，通过自己亲手实现这些编解码器的核心逻辑，来进一步加深对Netty的理解，这种感觉还是挺奇妙的。

## 2. Netty解决黏包/拆包问题的通用编解码器

## 总结


